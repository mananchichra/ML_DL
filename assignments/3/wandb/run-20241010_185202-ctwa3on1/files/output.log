Layer 1 - delta shape: (32, 32)
Layer 1 - d_weights shape: (8, 32)
Layer 1 - d_biases shape: (1, 32)
Layer 0 - delta shape: (32, 64)
Layer 0 - d_weights shape: (8, 64)
Layer 0 - d_biases shape: (1, 64)
d_weights[0].shape: (8, 64), self.weights[0].shape: (8, 64)
d_weights[1].shape: (8, 32), self.weights[1].shape: (64, 32)
Traceback (most recent call last):
  File "q2_6.py", line 182, in <module>
    mlp.fit(X_train.values, y_train)
  File "/home/mananchichra/Downloads/SMAI_ASSIGNMENT/models/MLP/MLPMC.py", line 350, in fit
    self.backward(X_batch, y_batch, output)
  File "/home/mananchichra/Downloads/SMAI_ASSIGNMENT/models/MLP/MLPMC.py", line 324, in backward
    self.weights[i] -= self.learning_rate * d_weights[i]
ValueError: operands could not be broadcast together with shapes (64,32) (8,32) (64,32)
